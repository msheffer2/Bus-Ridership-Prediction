---
title: "README"
author: "Matthew Sheffer"
date: "February 9, 2017"
output: md_document
tables: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bus Ridership Prediction for Predictive Analytics Capstone

Intro text

### R syntax files I used to generate the material necessary for the repo:

-----

* [01 - Data Integration & Missing Imputation Codeup.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/01%20-%20Data%20Integration%20%26%20Missing%20Imputation.R) -- Original data codeup spanned several data files and syntax files that were used to coordinate the codeup across different individuals and different data collection efforts.  This file begins with the completed "coded data" and uses it to integrate into a single dataset usable for prediction..
* [02 - EDA & Model Prep.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/02%20-%20EDA%20%26%20Model%20Prep.R) -- This syntax cycles through each of the predictors and creates graphs of its distrubtion with ridership.  The purpose of this was not only to perform EDA but to also examine if data transformations might not be helpful.  The data is then prepped for analysis by checking for Zero Variance variables and highly correlated predictors, as well as by creating training and testing partitions needed.
* [03 - Model 25.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/03%20-%20Model%2025.R) -- Generates the original winning model out of the 48 competing models:  bagged trees based on untransformed data.
* [04 - Plotting Data.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/04%20-%20Plotting%20Data.R) -- This is only an excerpt from the original file that plots presicion estimates for the competiting models.
* [05 - Improving Model 25.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/05%20-%20Improving%20Model%2025.R) -- This syntax file revises Model #25 to see if its possible to improve on current predictions but with fewer predictors.  Revised #25 uses just 12 predictors to beat out the current model but with far more manageable predictors for use later.
* [06 - Post Revision Plots.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/06%20-%20Post%20Revision%20Plots.R) -- Comparing the case RMSE for Model #25 to the original method and revised #25.  Also plots the variable importances for the revised model.
* [fit_assess.R](https://github.com/msheffer2/Bus-Ridership-Prediction/blob/master/fit_assess.R) -- a wrapper function to calculate the peformance metrics required to compare the models.  It calculates prediction accuraces via RMSE for training, testing, and test case data sets.

#### Technical Notes:

* The work shown here is less "replicable" than some of the other repos because of the need to alter all the original data and to avoid showing lots of rather uninteresting syntax showing multiple modelling efforts.  Instead, I provide pre-cleaned or pre-outputted data to streamline this process.  The random forest predictions show here, though, should be completely reproducible.

### Analytical Highlights

-----

```{r, echo=FALSE, message=F, warning=F}
library(dplyr)
library(ggplot2)
library(gplots)
```

x

##### Figure 1: Comparing the predictive accuracy of the 48 models tested
```{r, echo=FALSE, fig.width=6, fig.height=6}
load("./output/summary.Rdata")

colors <- c(
  "#f1eef6", "#f1eef6", "#f1eef6", "#f1eef6", "#f1eef6", "#f1eef6", 
  "#d0d1e6", "#d0d1e6", "#d0d1e6", "#d0d1e6", "#d0d1e6", "#d0d1e6", 
  "#a6bddb", "#a6bddb", "#a6bddb", "#a6bddb", "#a6bddb", "#a6bddb", 
  "#74a9cf", "#74a9cf", "#74a9cf", "#74a9cf", "#74a9cf", "#74a9cf", 
  "#FF0000", "#FF0000", "#FF0000", "#FF0000", "#FF0000", "#FF0000", 
  "#3690c0", "#3690c0", "#3690c0", "#3690c0", "#3690c0", "#3690c0", 
  "#0570b0", "#0570b0", "#0570b0", "#0570b0", "#0570b0", "#0570b0", 
  "#034e7b", "#034e7b", "#034e7b", "#034e7b", "#034e7b", "#034e7b"
)

# Plot
tplot <- ggplot(summary, aes(x=reorder(Name, Rrun), y=Projected)) + 
  geom_bar(position=position_dodge(), stat="identity", colour=colors, fill=colors) + 
  labs(x="Model", title="", y="Precision (RMSE) - Lower is Better") + 
  scale_y_continuous(limits=c(0,3200), breaks=seq(0,3000,500)) + 
  geom_text(aes(label = lab_pro, y= Projected), angle=90, nudge_y=150) + 
  geom_hline(aes(yintercept=818.1), size=2, colour="#FFC125") +
  annotate("text", x=29, y=2000, label="Test Cases RMSE: 818.1", color="#FFC125", hjust=-.1) +
  annotate("segment", x=29, xend=27, y=2000, yend=1000, color="#FFC125", size=1, arrow=arrow()) +
  annotate("text", x=1, y=0, label="Lasso Models", hjust=.05, vjust=1) +
  annotate("text", x=7, y=0, label="CART Models", hjust=.05, vjust=2.5) +
  annotate("text", x=13, y=0, label="Cubist Models", hjust=.05, vjust=1) +
  annotate("text", x=19, y=0, label="Random Forest", hjust=.05, vjust=2.5) +
  annotate("text", x=25, y=0, label="Bagged Trees", hjust=.05, vjust=1) +
  annotate("text", x=31, y=0, label="GBM Models", hjust=.05, vjust=2.5) +
  annotate("text", x=37, y=0, label="SVM Models", hjust=.05, vjust=1) +
  annotate("text", x=43, y=0, label="Neural Network", hjust=.05, vjust=2.5) +
  theme(text = element_text(size=15), 
        panel.grid.minor=element_blank(),
        panel.grid.major=element_blank(),
        axis.ticks=element_blank(),
        axis.text.x = element_text(angle=90, vjust=.3),
        panel.background = element_blank(),
        legend.position="none")
tplot
```

x

#### Model Function for Random Forest

-----

x

##### Figure 2:  Model #25, Revised Model #25, & Historical Prediction Precision
```{r, echo=FALSE, fig.width=6, fig.height=6}
dat <- data.frame(c(195, 818.1, 810.6))
dat <- cbind(dat, c("Model 25","Current Model", "Model 25 Revised"))
names(dat) <- c("Train", "Model")
dat$lab <- as.character(dat$Train)

colors <- c(
  "#FFC125", "#0570b0", "#FF0000"
)

tplot <- ggplot(dat, aes(x=Model, y=Train, fill=Model)) + 
  geom_bar(position="dodge", stat="identity") + 
  scale_fill_manual(values=colors) +
  labs(x="", title="", y="Precision (RMSE) - Lower is Better") + 
  scale_y_continuous(limits=c(0,1000), breaks=seq(0,1000,500)) + 
  geom_text(aes(label = lab, y= Train), nudge_y=50, hjust=0) + 
  #geom_text(aes(label = lab1, y= Train), nudge_y=50, hjust=1.5) + 
  theme(text = element_text(size=15), 
        panel.grid.minor=element_blank(),
        panel.grid.major=element_blank(),
        axis.ticks=element_blank(),
        legend.position="none")
tplot
```

x

##### Figure 3:  Model #25 Revised Model Importance
```{r, echo=FALSE, fig.width=6, fig.height=6}
load("./output/mod25_rev.Rdata")

imp <- caret::varImp(mod25_rev$fit) %>%
  tibble::rownames_to_column() %>%
  rename(Variable=rowname) %>%
  arrange(desc(Overall))

imp$Variable <- c("Day of Service", "High Frequency Line", "Personal Crime Index (Sum)",
                  "Weighted Crime Index", "Line of Service", "Number of Retail Jobs",
                  "Average Walk Score", "Population Size along Line", "Number of Non-retail Jobs",
                  "Number of Vacant Dwelling Unites (Sum)", "Minimum Walk Score", "Season")

imp$Importance <- round(imp$Overall, digits=1)
imp <- select(imp, Variable, Importance) %>%
  mutate(order=nrow(imp):1,
         lab=as.character(Importance))

#Plot

iplot <- ggplot(imp, aes(x=reorder(Variable, order), y=Importance)) + 
  geom_bar(stat="identity", fill="darkmagenta") + coord_flip() + 
  geom_text(aes(label = lab, y= Importance), hjust=-.25, vjust=.3) + 
  labs(x="", title="", y="Variable Importance (OOB Error)") +
  theme(text = element_text(size=15), 
        panel.grid.minor=element_blank(),
        panel.grid.major=element_blank(),
        axis.ticks=element_blank(),
        legend.position="none")
iplot
```

x


The last task, then, is to score the database so that the client can use the predictions to plan promotional or sales pieces targetted toward each segment.  The "positive" sample bias noted above is even more obvious now once the estimated population sizes are shown in Table 2.  Segment 4 was a critical group and represented about 13% of the sample but in reality, this group of individuals likely represents closer to 3% of the target universe for the client (as an aside, this low percentage was not a surprise to the client).  Similarly, Segment 5 represent the lowest opportunity for the client; it was only about 22% of the sample but turned out to be a disappointing 41% of the target universe.

-----

##### References Cited
1. Kuhh, Max. and Kjell Johnson.  2013.  Applied Predictive Modelling.  New York:  Springer.
2. Kohavi, Ron.  1995.  "A Study of Cross-Validation and Bootstraop for Accuracy Estimation and Model Selection".  Presented at the International joint Conference on Artificial Intelligence, Montreal, Quebec.
3. Borra, Simone. and Agostino Di Ciaccio.  2010.  "Measuring the Prediction Error:  A Comparison of Cross-Validation, Bootstrop, and Covariance Penalyt Methods".  Computational Statistics and Data Analysis.  54: 2976-2989.

